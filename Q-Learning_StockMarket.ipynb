{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arhum Zafar\n",
    "<br>\n",
    "<br>\n",
    "A brief attempt of using reinforcement learning strategies to make a quick trading agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu==2.0.0.alpha0|\n",
    "from src.config.config import Config\n",
    "from src.db_writer.db import DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as data_reader\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the trading network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Trader():\n",
    "  \n",
    "  def __init__(self, state_size, action_space=3, model_name=\"AITrader\"): #Stay, Buy, Sell\n",
    "    \n",
    "    self.state_size = state_size\n",
    "    self.action_space = action_space\n",
    "    self.memory = deque(maxlen=2000)\n",
    "    self.inventory = []\n",
    "    self.model_name = model_name\n",
    "    \n",
    "    self.gamma = 0.95\n",
    "    self.epsilon = 1.0\n",
    "    self.epsilon_final = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    \n",
    "    self.model = self.model_builder()\n",
    "    \n",
    "  def model_builder(self):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=32, activation='relu', input_dim=self.state_size))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=self.action_space, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "    \n",
    "    return model\n",
    "  \n",
    "  def trade(self, state):\n",
    "    \n",
    "    if random.random() <= self.epsilon:\n",
    "      return random.randrange(self.action_space)\n",
    "    \n",
    "    actions = self.model.predict(state, verbose=0)\n",
    "    return np.argmax(actions[0])\n",
    "  \n",
    "  \n",
    "  def batch_train(self, batch_size):\n",
    "    \n",
    "    batch = []\n",
    "    for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
    "      batch.append(self.memory[i])\n",
    "      \n",
    "    for state, action, reward, next_state, done in batch:\n",
    "      reward = reward\n",
    "      if not done:\n",
    "        reward = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "        \n",
    "      target = self.model.predict(state, verbose=0)\n",
    "      target[0][action] = reward\n",
    "      \n",
    "      self.model.fit(state, target, epochs=1, verbose=0)\n",
    "      \n",
    "    if self.epsilon > self.epsilon_final:\n",
    "      self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "#Price Format Function\n",
    "def stocks_price_format(n):\n",
    "  if n < 0:\n",
    "    return \"- $ {0:2f}\".format(abs(n))\n",
    "  else:\n",
    "    return \"$ {0:2f}\".format(abs(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(stock_name):\n",
    "  \n",
    "  #Complete the dataset loader function\n",
    "  config = Config()\n",
    "  db = DB(config)\n",
    "  funding_data = pd.read_sql('SELECT * FROM `binance`.funding_data;', con=db.con)\n",
    "  funding_data = funding_data.sort_values(by='timestamp')\n",
    "  funding_data\n",
    "\n",
    "  # dataset = data_reader.DataReader(stock_name, data_source=\"yahoo\")\n",
    "  \n",
    "  # start_date = str(dataset.index[0]).split()[0]\n",
    "  # end_date = str(dataset.index[-1]).split()[0]\n",
    "  \n",
    "  close = funding_data['mark_price']\n",
    "  return close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_creator(data, timestep, window_size):\n",
    "  \n",
    "  starting_id = timestep - window_size + 1\n",
    "  # print(starting_id)\n",
    "  # print(data[0])\n",
    "  if starting_id >= 0:\n",
    "    windowed_data = data[starting_id:timestep+1]\n",
    "  else:\n",
    "    windowed_data = - starting_id * [data[0]] + list(data[0:timestep+1])\n",
    "  windowed_data = list(windowed_data)\n",
    "  state = []\n",
    "  for i in range(window_size - 1):\n",
    "    state.append(sigmoid(windowed_data[i+1] - windowed_data[i]))\n",
    "    \n",
    "  return np.array([state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a dataset for a stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"AAPL\"\n",
    "data = dataset_loader(stock_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "window_size = 10\n",
    "episodes = 1000\n",
    "\n",
    "batch_size = 32\n",
    "data_samples = len(data) - 1 - window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 32)                352       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,171\n",
      "Trainable params: 11,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trader = AI_Trader(window_size)\n",
    "trader.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 123/116849 [06:38<100:36:36,  3.10s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "  \n",
    "  print(\"Episode: {}/{}\".format(episode, episodes))\n",
    "  \n",
    "  state = state_creator(data, 0, window_size + 1)\n",
    "  \n",
    "  total_profit = 0\n",
    "  trader.inventory = []\n",
    "  \n",
    "  for t in tqdm(range(data_samples)):\n",
    "    \n",
    "    action = trader.trade(state)\n",
    "    # print((t+1, window_size + 1))\n",
    "    next_state = state_creator(data, t+1, window_size + 1)\n",
    "    reward = 0\n",
    "    \n",
    "    if action == 1: #Buying\n",
    "      trader.inventory.append(data[t])\n",
    "      print(\"AI Trader bought: \", stocks_price_format(data[t]))\n",
    "      \n",
    "    elif action == 2 and len(trader.inventory) > 0: #Selling\n",
    "      buy_price = trader.inventory.pop(0)\n",
    "      \n",
    "      reward = max(data[t] - buy_price, 0)\n",
    "      total_profit += data[t] - buy_price\n",
    "      print(\"AI Trader sold: \", stocks_price_format(data[t]), \" Profit: \" + stocks_price_format(data[t] - buy_price) )\n",
    "      \n",
    "    if t == data_samples - 1:\n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "      \n",
    "    trader.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "      print(\"########################\")\n",
    "      print(\"TOTAL PROFIT: {}\".format(total_profit))\n",
    "      print(\"########################\")\n",
    "    \n",
    "    if len(trader.memory) > batch_size:\n",
    "      trader.batch_train(batch_size)\n",
    "      \n",
    "  if episode % 10 == 0:\n",
    "    trader.model.save(\"ai_trader_{}.h5\".format(episode))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether you have a GPU or not, the loop time will vary. As the loop iterates, you will notice how the network makes stronger predictions over time; reinforcing its behavior over while iterating, yielding greater profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
